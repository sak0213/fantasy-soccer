{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sim_classes import TeamStatGen, TeamProfile, GameManager\n",
    "from config import game_setup, local_host, local_name, local_port, local_user\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import warnings\n",
    "from creds import local_db_password\n",
    "warnings.simplefilter('ignore')\n",
    "conn = psycopg2.connect(dbname=local_name, user =local_user, host=local_host, password=local_db_password, port =local_port)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = '2023'\n",
    "iterations = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_games_sql = f\"\"\"\n",
    "select \n",
    "    id,\n",
    "    team_home,\n",
    "    team_away,\n",
    "    score_home,\n",
    "    score_away,\n",
    "    season_round\n",
    "from ffl.fixtures \n",
    "where season = '{season}'\n",
    "\"\"\"\n",
    "real_results_df = pd.read_sql(fetch_games_sql, conn)\n",
    "# real_results_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results(df, id_passthrough):\n",
    "    home_win = 0\n",
    "    away_win = 0\n",
    "    tie_game = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row['score_home'] > row['score_away']:\n",
    "            home_win += 1\n",
    "        elif row['score_home'] < row['score_away']:\n",
    "            away_win += 1\n",
    "        else:\n",
    "            tie_game += 1\n",
    "    \n",
    "    return id_passthrough, home_win, away_win, tie_game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sim block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_schedule = real_results_df['id']\n",
    "sim_results_df = pd.DataFrame(columns=['id', 'home_win', 'away_win', 'tie_game'])\n",
    "\n",
    "for i in simulation_schedule:\n",
    "    team_stats = TeamStatGen(i)\n",
    "    home_team = TeamProfile(team_stats.configure_team_stats('home'))\n",
    "    away_team = TeamProfile(team_stats.configure_team_stats('away'))\n",
    "    result_df = pd.DataFrame(columns=['score_home', 'score_away'])\n",
    "    for j in range(iterations):\n",
    "        game = GameManager(home_team, away_team, game_setup)\n",
    "        result_df.loc[len(result_df)] = game.run_game('scores') #type: ignore\n",
    "\n",
    "    sim_results_df.loc[len(sim_results_df)] = (aggregate_results(result_df, i)) #type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_sim_result = []\n",
    "assess_real_result = []\n",
    "assess_baseline_result = []\n",
    "for index, row, in sim_results_df.iterrows():\n",
    "    if row['home_win'] > row['away_win'] and row['home_win'] > row['tie_game']:\n",
    "        assess_sim_result.append('win')\n",
    "    elif row['away_win'] > row['home_win'] and row['away_win'] > row['tie_game']:\n",
    "        assess_sim_result.append('loss')\n",
    "    elif row['tie_game'] > row['home_win'] and row['tie_game'] > row['away_win']:\n",
    "        assess_sim_result.append('tie')\n",
    "    else:\n",
    "        assess_sim_result.append('tie')\n",
    "\n",
    "for index, row in real_results_df.iterrows():\n",
    "    assess_baseline_result.append('win')\n",
    "\n",
    "    if row['score_home'] > row['score_away']:\n",
    "        assess_real_result.append('win')\n",
    "    elif row['score_away'] > row['score_home']:\n",
    "        assess_real_result.append('loss')\n",
    "    elif row['score_home'] == row['score_away']:\n",
    "        assess_real_result.append('tie')\n",
    "def generate_conf_matrix(pred_results, real_results):\n",
    "    pw_tw = 0\n",
    "    pw_tl = 0\n",
    "    pw_tt = 0\n",
    "    pl_tw = 0\n",
    "    pl_tl = 0\n",
    "    pl_tt = 0\n",
    "    pt_tw = 0\n",
    "    pt_tl = 0\n",
    "    pt_tt = 0\n",
    "\n",
    "    if len(real_results) == len(pred_results):\n",
    "        for game in range(len(pred_results)):\n",
    "            if pred_results[game] == 'win' and real_results[game] == 'win':\n",
    "                pw_tw +=1\n",
    "            elif pred_results[game] == 'win' and real_results[game] == 'loss':\n",
    "                pw_tl +=1\n",
    "            elif pred_results[game] == 'win' and real_results[game] == 'tie':\n",
    "                pw_tt +=1\n",
    "            elif pred_results[game] == 'loss' and real_results[game] == 'win':\n",
    "                pl_tw +=1\n",
    "            elif pred_results[game] == 'loss' and real_results[game] == 'loss':\n",
    "                pl_tl +=1\n",
    "            elif pred_results[game] == 'loss' and real_results[game] == 'tie':\n",
    "                pl_tt +=1\n",
    "            elif pred_results[game] == 'tie' and real_results[game] == 'win':\n",
    "                pt_tw +=1\n",
    "            elif pred_results[game] == 'tie' and real_results[game] == 'loss':\n",
    "                pt_tl +=1\n",
    "            elif pred_results[game] == 'tie' and real_results[game] == 'tie':\n",
    "                pt_tt +=1\n",
    "        \n",
    "        print(f'Total Accuracy:{(pw_tw + pl_tl + pt_tt) / len(real_results)}')\n",
    "        # zero division error only intended to manage the baseline, \"home always wins\", data\n",
    "        try:\n",
    "            print(f'Win - Accuracy: {pw_tw / (pw_tw + pw_tl + pw_tt)} -- Recall: {(pw_tw / (pw_tw + pl_tw + pt_tw))}')\n",
    "        except(ZeroDivisionError):\n",
    "            print('Win Accuracy: 0 -- Recall: 0')\n",
    "        try:\n",
    "            print(f'Loss - Accuracy: {pl_tl/ (pl_tw + pl_tl + pl_tt)} -- Recall: {(pl_tl / (pw_tl + pl_tl + pt_tl))}')\n",
    "        except(ZeroDivisionError):\n",
    "            print('Loss Accuracy: 0 -- Recall: 0')\n",
    "        try:\n",
    "            print(f'Tie - Accuracy: {pt_tt/ (pt_tw + pt_tl + pt_tt)} -- Recall: {(pt_tt / (pw_tt + pl_tt + pt_tt))}')\n",
    "        except(ZeroDivisionError):\n",
    "            print('Tie Accuracy: 0 -- Recall: 0')\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            'labels': ['true_win', 'true_loss', 'true_tie'],\n",
    "            'pred_win':[pw_tw, pw_tl, pw_tt],\n",
    "            'pred_loss':[pl_tw, pl_tl, pl_tt],\n",
    "            'pred_tie':[pt_tw, pt_tl, pt_tt]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy:0.4921052631578947\n",
      "Win - Accuracy: 0.5055762081784386 -- Recall: 0.7771428571428571\n",
      "Loss - Accuracy: 0.4897959183673469 -- Recall: 0.3902439024390244\n",
      "Tie - Accuracy: 0.23076923076923078 -- Recall: 0.036585365853658534\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>pred_win</th>\n",
       "      <th>pred_loss</th>\n",
       "      <th>pred_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>true_win</td>\n",
       "      <td>136</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>true_loss</td>\n",
       "      <td>71</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>true_tie</td>\n",
       "      <td>62</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      labels  pred_win  pred_loss  pred_tie\n",
       "0   true_win       136         33         6\n",
       "1  true_loss        71         48         4\n",
       "2   true_tie        62         17         3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_cm = generate_conf_matrix(assess_sim_result, assess_real_result)\n",
    "sim_cm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy:0.4605263157894737\n",
      "Win - Accuracy: 0.4605263157894737 -- Recall: 1.0\n",
      "Loss Accuracy: 0 -- Recall: 0\n",
      "Tie Accuracy: 0 -- Recall: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>pred_win</th>\n",
       "      <th>pred_loss</th>\n",
       "      <th>pred_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>true_win</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>true_loss</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>true_tie</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      labels  pred_win  pred_loss  pred_tie\n",
       "0   true_win       175          0         0\n",
       "1  true_loss       123          0         0\n",
       "2   true_tie        82          0         0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_cm = generate_conf_matrix(assess_baseline_result, assess_real_result)\n",
    "base_cm.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
